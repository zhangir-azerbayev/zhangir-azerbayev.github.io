---
layout: post
title: Reflections on Timothy Gowers' Manifesto
---

This morning, eminent mathematician Timothy Gowers announced he was building an automated theorem proving team and
released a [54 page manifesto](https://drive.google.com/file/d/1-FFa6nMVg18m1zPtoAQrFalwpx2YaGK4/view). I was thrilled 
by this announcement: Gowers is one of my mathematical heros. His contributions include the insight that combinatorial
methods can be used to study functional analysis, which revolutionzed our understanding of the geometry of Banach spaces, 
for example by resolving the Banach-hyperplane conjecture. He also applied analytic methods to combinatorial problems, 
most famously in his alternate proof of Szemeredi's theorem. Aside from my personal love of Gowers' work, I am always
happy to see mainstream pen-and-paper mathematicians embrace computer-assisted theorem proving, since it is primarily 
mainstream mathematicians who convince other mainstream mathematicians to learn about computer theorem proving. 

Let me briefly summarize Gowers' program. He begins with the observation that the general problem of furnishing a proof
given a well-formed mathematical proposition is undecidable. So how is that humans prove theorems? The key is that 
humans are not interested in proving arbitrary theorems plucked from &#120121;, the entire space statement-proof pairs. 
Instead mathematicians are interested in a subspace of &#120121;, which we'll call &#120132;, that comprises
all intersting mathematical statements paired with a satisfying proof, where a statement may be interesting because it has a property like 
"is beautiful", "is a natural question about elementary objects", or "is party of a research program". A proof is satisfying
to the extent that it provides genuine understanding, rather than just being a formal deriviation. Gowers is 
interested in accomplishing three goals: 

1. Figure out what distinguishes &#120121; from &#120132;. 
2. Figure out why it is feasible for humans to find a proof P of statement S when (S,P) belongs to &#120132;. 
3. Use the insights from answering (1) and (2) to build a GOFAI human-level automated theorem prover (ATP) that produces human-friendly proofs. 

My expectation is that efforts into (1) and (2) will be tremendousfly fruitful---these are the most profound questions
about the relationship between mathematics and mathematicians, and as far as I know they are barely studied. However, 
I doubt that the answers to (1) and (2) will translate into progress on (3), and furthermore, that any attempt to build 
a GOFAI human-level ATP has an exceedingly low probability of success. 

A long history of AI research has shown that building systems by enumerating the rules they are supposed to follow does
not scale to near human-level performance on intelligent tasks. After testing such a system, the researcher realizes
there is an edge case they forgot to account for, and they modify their system to add a rule for the edge 
case. But iterating this process of trial-and-error does not converge: the problem is like a Hydra, where for every 
new rule you add you discover a panoply of new edge cases the new rules don't cover. This pattern has unfolded in 
vision, board games, language, speech processing, information retrieval, and just about every other field of AI. 

Instead of trying to figure out the algorithm for an intelligent task, what has instead succeeded is writing a 
*meta-algorithm* that searches program-space in order to learn the algorithm for the intelligent task. Why does 
typically GOFAI fail and learning often succeed? I suspect because of something like the following: call an algorithm
*illegible* if it is so unintelligble and fiendishly complicated that it would be next to impossible for a human, with
their limited mental powers, to discover and implement. The only hope for solving tasks that require illegible algorithms
is some kind of program-search, i.e machine learning. 

The history of AI suggests that vision, speech and language processing, and board-game playing demand unintelligble 
algorithms. We can reformulate our question of whether a GOFAI program inspired by studying human mathematical reasoning
can be a human-level ATP as follows: is the human theorem-proving algorithm (henceforth HTPA) unintelligble? I think there are strong grounds to believe it is. First, it certainly seems that playing Go at a human level requires
an illegible algorithm, and I'd expect that the game tree of Go is a far less complex object than the collection
of proof search trees of statement in &#120132;.

