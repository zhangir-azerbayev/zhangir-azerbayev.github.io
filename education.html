<html>

<head>

  <title>Azerbayev - Essays and Internet Writing</title>

  <meta http-equiv="Content-Type" content="text/html"
  charset=iso-8859-1>

  <meta name="description" content="Zhangir Azerbayev research page">

  <meta name="keywords" content="deep learning, neural networks, 
  machine learning, proof assistants, automated reasoning">

  <link rel='stylesheet' type='text/css' href='home.css'>

  <link rel="icon" href="favicon.ico" type="image/x-icon" />
 
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
  
  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro' rel='stylesheet' type='text/css'>
</head>

<div id="mymenu">

<p><a href="index.html">Welcome</a></p>
<p><a href="papers.html">Papers</a></p>
<p><a href="talks.html">Talks</a></p>
<p><a href="software.html">Software</a></p>
<p><a href="blog.html">Blog</a></p>
</div>

<div id="main">
<p>
    Because deep learning is a young field and the field's current paradigm is younger still,
    the question of what to study in order to become a deep learning researcher is a difficult one.

    Short of being able to give a dispositive answer, what follows is the list of books that constitutes 
    the foundation of my technical background, and thus, is one viable way to "pre-train" yourself 
    for deep learning research.
</p>

<div id="h1"><b>Mathematics</b></div>
<p>
    Apart from a basic grasp of analysis and linear algebra, I don't know how much studying pure mathematics
has helped me in deep learning. I would like for it to be the case that studying mathematics endowed me 
with superior problem solving ability, an inclination towards rigor, and a high degree of aesthetic discernment. 
But it is equally possible that I merely find mathematics fun and am fishing for a rational justification
of all the time I spent on it.
</p>

Algebra:
<ul>
    <li>
        <i>Linear Algebra Done Right</i>, Sheldon Axler. Basically a perfect course on linear algebra.
        Supplement with chapters 1, 3-5 of Michael Artin's <i>Algebra</i> if you would like some practice with
        matrices.
    </li>
    <li>
        <i>Abstract Algebra</i>, Dummit and Foote, chapters 1-9. The applications to number theory, such as
        the proof of Fermat's two-square theorem via the Gaussian integers, are the highlight of this text.
    </li>
</ul>

Analysis:
<ul>
    <li>
        <i>Principles of Mathematical Analysis</i>, Walter Rudin, chapters 1-7. There are several other
        great introductory analysis texts, such as Tao's or Pugh's.
    </li>
    <li>
        I took a fantastic course on Lebesgue Theory and Fourier Analysis from Hee Oh at Yale University.
        The course did not follow a text precisely, but chapters 1-5 of Stein and Shakarchi's 
        <i>Measure Theory, Integration, and Hilbert Spaces</i> matches the content closely.
    </li>
    <li>
        <i>Calculus on Manifolds</i>, Spivak. Chapters 4-5 form the bridge from calculus to geometry.
    </li>
</ul>


<div id="h1"><b>Applied Mathematics and CS Theory</b></div>
<p>Every text listed from here on is directly relevant to deep learning.</p>

<p>Usually, <i>learning theory</i> means statistical learning theory, which
deals in concepts such as bias-variance tradeoff, generalization bounds, and kernel methods.
However, to me, the true theory of learning is <i>algorithmic probability</i>, where among the important
concepts are Solomonoff induction, Kolmogorov complexity, minimum description length, and prequential coding.
The best introduction to algorithmic probability I know of is the 
<a href="https://arxiv.org/abs/math/0406077">Tutorial Introduction to the
Minimum Description Length Principle.</a></p>


</div>

</html>